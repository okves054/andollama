# ğŸ¤– Andollama
Run Ollama locally on your old Android phone â€” no internet required! ğŸš€

### ğŸŒŸ What Is This?
Ever wished you could run LLMs like **Ollama** directly on your Android device â€” even an old one â€” completely offline?
**Andollama** makes it happen ğŸ¦™ğŸ“±
<p align="center">
  <img src="screenshot.jpg" alt="Andollama frontend" width="200">
</p>

### âš™ï¸ Requirements

#### ğŸ§© Step 1 â€” Install Termux
Download the **Termux** APK from the official GitHub:
ğŸ‘‰ [termux/termux-app/releases]()
Open Termux once installed â€” youâ€™ll be living in the terminal from here on ğŸ‘¨â€ğŸ’»
#### ğŸª„ Step 2 â€” Set Up Your Environment
Run these commands one by one in Termux:
```
pkg update
pkg upgrade
pkg install -y python python-pip git golang rust
```
#### ğŸ§  Step 3 â€” Clone and Build Ollama
```
git clone --depth=1 https://github.com/ollama/ollama.git
cd ollama
go generate ./...
go build .
./ollama --version
cd ..
```
(Grab a coffee â€” this part can take a while depending on your phone â˜•)
#### ğŸ’¾ Step 4 â€” Clone Andollama & install Dependencies
```
git clone --depth=1 https://github.com/okves054/andollama.git
python -m venv venv && source venv/bin/activate
cd andollama
pip install -U pip flask requests
cd ..
```
#### ğŸ”¥ Step 5 â€” Start server & Launch
```# Start Ollama server
./ollama/ollama serve &

# Run a small model
./ollama/ollama run qwen2.5:0.5b

# Start the web interface
python andollama/run.py
```
#### ğŸŒ Step 6 â€” Use It!
Open your phoneâ€™s browser and visit:
ğŸ‘‰ http://localhost:5000
Boom ğŸ’¥ â€” youâ€™re chatting with a local AI running right on your Android device.
No cloud, no data leaks, no limits. ğŸ§˜â€â™‚ï¸
#### ğŸ’¡ Pro Tips
Want a different model? Replace **qwen2.5:0.5b** with any Ollama-supported model from [here](https://ollama.com/search).
To stop servers, just hit Ctrl + C or close Termux.
You can pin Termux to keep it running in the background ğŸ”‹
